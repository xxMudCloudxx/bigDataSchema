好的，我们已经明确了老师的所有要求，特别是“HBase或MyCat二选一”、“虚拟节点=数据分片”和“AI扩充数据”这几点。

这太好了！我们的技术路线现在100%清晰了。**我们正式决定：放弃HBase，集中精力主攻 MyCat 数据分片，并升级数据模拟方案。**

这是我们最新的、也是最终的**《课设项目策划书 v1.1 (最终版)》**。它已经集成了我们所有的讨论和老师的最新反馈，是我们4个人未来几周的“唯一行动指南”。

------



# 课设项目策划书：《高速公路实时车流监控与套牌车预警平台》v1.1



## 1. 🎯 项目概述

本项目旨在利用你提供的 `交通流量数据字段说明.png` 所示的高速公路卡口过车数据，构建一个集**数据存储、实时监控、交互查询、实时预警**于一体的大数据管理平台。

我们将复用往届学长的经典技术架构（Flume+Kafka+Flink，参考 `image_abe064.png`），但业务逻辑将**完全围绕我们的“卡口数据”和“套牌车检测”扩展要求展开**。

**核心功能 (已确认):**

1. **数据大屏**：动态展示全市车流、卡口流量排名、24小时流量曲线、车辆来源地图炮。（参考 `刘冰峰-前端.pdf`）
2. **交互查询**：按卡口、车牌、时间等多维度组合查询过车记录。（参考 `高成-前端.pdf`）
3. **实时预警 (项目亮点)**：实时检测“同一车牌在短时间内出现在不同卡口”的套牌车嫌疑行为。（参考 `大数据存储案例设计要求.doc`）



## 2. 💻 核心技术栈 (最终版)



- **数据采集**：Python (模拟数据流 + **AI数据修复与扩充**)
- **数据缓存**：Kafka
- **数据存储**：MySQL 5.7 (**单机双节点/数据分片**), **MyCat** (核心), Redis
- **实时计算**：Flink
- **离线计算**：Spark ML
- **后端服务**：Python Flask
- **前端展示**：Vue / React, Echarts
- **(已废弃)**：HBase



## 3. 👥 4人团队分工 (RACI) - (最终版)

| **角色**               | **负责人** | **核心职责 (Responsible)**                                   | **上游**   | **下游**       |
| ---------------------- | ---------- | ------------------------------------------------------------ | ---------- | -------------- |
| **前端 & 管道 (全栈)** | **同学A ** | 1. **数据源**：Python模拟流 -> Flume -> Kafka (`traffic-raw`) <br />2. **前端**：开发数据大屏与交互查询页面 (Vue/React) | 无         | B, C, D        |
| **存储架构师 (主)**    | **同学 B** | 1. **数据库**：在 `db-app` 搭建 MySQL双节点 + MyCat 分片 <br />2. **入库ETL**：编写 Flink Job (Java)，消费 Kafka -> Sink to MyCat | 我 (Kafka) | D (MyCat)      |
| **算法工程师 (辅)**    | **同学 C** | 1. **实时报警**：编写 Flink Job (Java)，检测套牌车 -> Sink to Redis <br />2. **离线模型**：Spark ML 训练预测模型 -> 保存模型文件 | 我 (Kafka) | D (Redis/模型) |
| **后端 API 开发**      | **同学 D** | 1. **统一接口**：开发 Flask Web 服务，统一提供查询、报警、预测 API <br />2. **对接存储**：负责连接 MyCat 查询数据，连接 Redis 读取报警 | B, C       | 我 (前端)      |

## 4. 🛰️ 基础设施部署方案 (2台服务器 - 虚拟节点版) - (已确认)

**服务器 1: `pipeline-algo` (计算与消息中心)**

- **负责人**：**A** (环境搭建), **B & C** (提交 Flink Jar包)
- **运行服务**：
  - **Zookeeper & Kafka**: (A负责) 消息中间件，Topic: `traffic-raw`
  - **Flume**: (A负责) 日志采集
  - **Flink Cluster**: (B & C 使用) 运行 `KafkaToMyCat` 和 `TaopaiDetect` 任务
  - **Spark**: (C 使用) 离线训练任务



 **服务器 2: `db-app` (存储与服务中心)**

- **负责人**：**B** (DB/MyCat), **C** (Redis), **D** (Flask)
- **运行服务**：
  - **MySQL 实例 1**: 端口 `3306` (B负责)
  - **MySQL 实例 2**: 端口 `3307` (B负责，虚拟节点/分片)
  - **MyCat**: 端口 `8066` (B负责，核心分片入口)
  - **Redis**: 端口 `6379` (C负责)
  - **Flask API**: 端口 `5000` (D负责，统一后端接口)

------

## 5. 📅 详细任务清单 (Action Items)



### **🧑‍💻 同学A (前端 & 管道)**

1. **数据生产**：保证 `pipeline-algo` 上的 `simulate.py` 和 `traffic-raw` 主题 7x24小时稳定产出数据。
2. **前端开发**：根据 D 提供的 API 接口文档，开发数据大屏和查询页面。
3. **交付物**：`traffic-raw` (Kafka Topic), 前端网页。



### **🧑‍💻 同学 B (存储 & 入库)**

1. **MySQL 双实例**：在 `db-app` 上配置 `3306` 和 `3307` 两个 MySQL 实例。
2. **MyCat 分片**：配置 `schema.xml`，实现按 `KKMC` (卡口名称) 或 `SFZRKMC` 将数据分片到这两个 MySQL 实例。
3. **Flink 入库**：编写 Flink Java 代码，读取 `pipeline-algo` 的 Kafka，通过 JDBC (Druid池) 写入 `db-app` 的 MyCat (`8066`)。
   - *注意：你的 Flink 代码运行在 `pipeline-algo`，但写入的目标 IP 是 `db-app`。*



### **🧑‍💻 同学 C (算法 & 缓存)**

1. **Redis 部署**：在 `db-app` 上安装并启动 Redis。
2. **Flink 实时报警**：编写 Flink Java 代码（状态编程），检测套牌车（同车牌短时间出现在不同卡口），将报警信息 `LPUSH` 到 `db-app` 的 Redis List 中。
3. **Spark 预测**：在 `pipeline-algo` 上训练模型，并将模型文件传输给 D 同学（或者放在共享目录），供 Flask 加载。



### **🧑‍💻 同学 D (后端 API)**

1. **Flask 搭建**：在 `db-app` 上搭建 Flask 服务。
2. **业务接口**：连接本机的 MyCat (`localhost:8066`)，为前端提供 SQL 查询接口（流量排名、交互查询）。
3. **算法接口**：
   - 连接本机的 Redis (`localhost:6379`)，读取报警队列，提供给前端。
   - 加载 C 同学提供的 Spark/算法模型，提供流量预测 API。
4. **CORS 配置**：务必在 Flask 中配置跨域允许，因为A的前端是独立运行的。

## 6. 📖 详细执行计划 (Step-by-Step) v1.1

### 阶段 0：项目“握手协议” (第1天，我)

- **目标**：定义前后端“契约”，实现解耦。
- **行动 (我)**：A（前端）**必须立刻**编写一份 `API.md` 接口文档，发给同学 B 和 C。这份文档就是我们之前讨论的**《项目技术接口规范与交付物定义》**。
- **参考资料**：
  - `刘冰峰-前端.pdf` (第 9-12 页)：参考他为大屏图表定义了哪些数据格式。
  - `朱少行-后端.pdf` (第 9-13 页)：参考他的 API 设计（`solution` 类）。

### 阶段 1：数据管道打通 (同学A) - (v1.2 重大更新)[已完成]

(A已完成本阶段全部任务，数据管道已于 11-18 全面打通并转入后台 7x24h 运行。)

#### 运维详情 (A组交付物)

1. **数据源 (`6.simulate.py`)**: 已在后台 (`&`) 运行。
   - 已解决 `GBK` 编码问题 (原 `utf-8` 报错 `invalid continuation byte`)。
   - 已解决 `/var/log/traffic.log` 的 `chmod 666` 写入权限问题。
2. **日志轮替 (`logrotate`)**: 已配置 `/etc/logrotate.d/traffic-pipe`。
   - `daily` 轮替，`rotate 2` 保留 2 天数据。
   - `create 666 root root` 和 `su root root` 解决权限问题并确保 Flume 无缝衔接。
   - **(结论)**：`/var/log/traffic.log` **不会**撑爆磁盘。
3. **Flume (v1.9.0)**: 已在后台 (`nohup`) 运行。
   - `TAILDIR` Source 监控 `/var/log/traffic.log`。
   - `KafkaSink` 写入 `localhost:9092`。
   - 已解决 `mkdir logs` 使 `nohup` 日志重定向 生效。
4. **Kafka (v2.4.1)**: 已在后台 (`-daemon`) 运行。
   - **(关键优化)**：数据保留周期已从 7 天 INFO KafkaConfig values: ... log.retention.hours = 168 ... (kafka.server.KafkaConfig)] 优化为 2 天 (`log.retention.hours=48`)，防止磁盘占满。
5. **Zookeeper (v3.5.7)**: 已在后台运行。



#### 下游须知 (同学B, C)

1. **Kafka Broker 地址**: 你们的 `Flink` 作业 (运行于 `pipeline-algo`) 的连接地址**必须**是 `localhost:9092` INFO KafkaConfig values: ... listeners = PLAINTEXT://localhost:9092]。
2. **禁止使用**: (已废弃) **不要**使用公网IP。我们联调时已确认，这会导致 `LEADER_NOT_AVAILABLE` 致命错误。
3. **Topic**: 你们消费的 Topic 是 `traffic-raw`。

(原 阶段 1 策划内容（1.merge... 7.Flume...） 作为历史归档，保留在下面。)

- **目标**：提供一个稳定的、**经过AI修复、标准化和属性扩充的** Kafka Topic (`traffic-raw`)。

- **行动 (同学A)**：
  1. **数据准备 (1.merge_data.py)**：合并 `202312` 和 `202401` 文件夹下的所有 CSV/XLSX 文件，按 `GCSJ` 排序，输出 `merged_data.csv`。
  
  2. **数据分析 (2.kkmc_profiler.py)**：分析 `KKMC` 字段，输出 `kkmc_statistics.txt` 供人工决策。
  
  3. **数据标准化 (3.standardize.py)**：
     - **目标**：将数据转换为可用于分片 和 API 的“干净”数据。
     - **逻辑 (KKMC)**：将长中文卡口名 替换为唯一的 Codename (如 `S325_Suini...`)。
     - **逻辑 (HPZL)**：清理前导单引号 并将 `1` 统一为 `01`。
     - **产出 1**: `merged_data_standardized.csv` (给下游)
     - **产出 2**: `kkmc_dimension.csv` (交付给同学B，用于 MyCat 维度表)
  
  4. **AI修复/扩充 (4.preprocess.py)**：
     - **目标**：解决数据“脏”和“缺失”的问题。
     - **逻辑 (修复)**：修复无效 `HPHM` (如 `6***`)，并对 `HPZL` 和 `CLPPXH` 中 `"-"` 占位符进行“AI插补”。
     - **逻辑 (扩充)**：复制 12-17 号的数据，伪造 18-21 号的缺失数据。
     - **产出**: `master_data_cleaned.csv` (8 字段)。
  
  5. **AI特征工程 (5.expand_features.py)**：
     - **目标**：满足“AI扩充属性” 要求，为前端和算法 提供高价值特征。
     - **逻辑**: 从 `HPHM` 提取 `PROVINCE`，从 `GCSJ` 提取 `IS_WEEKEND` 和 `IS_PEAK_HOUR`，从 `HPZL` 映射 `VEHICLE_TYPE_NAME`。
     - **产出**: `master_data_expanded.csv` (12 字段)。
  
  6. **模拟脚本 (6.simulate.py)**：
     - **目标**：读取 12 字段的 `master_data_expanded.csv`，模拟“每秒50条” 的实时流。
     - **逻辑**: 替换 `GCSJ` 为当前时间，将 12 个字段 用 `\t` 分隔后写入 `/var/log/traffic.log`。
  
  7. **Flume 配置**：(不变) `TAILDIR` 监控 `/var/log/traffic.log`，Sink 到 `traffic-raw`。
  
- **参考资料**：
  - `杨研博-数据管道.pdf` (第 4-7, 13 页) 依然是 ZK, Kafka, Flume 的环境配置圣经。



### 阶段 2：存储架构搭建与入库 (第2-7天，同学B) - (重大更新)

这一阶段是整个项目的“地基”，你将在一台服务器 (`db-app`) 上通过**“虚拟节点”**技术模拟出一个分布式的数据库集群，并打通实时数据入库链路。

- **目标**：在 `db-app` 服务器上构建 MyCat 分片集群，并让 Flink 实时将 Kafka 数据写入其中。
- **工期**：第 2 - 5 天
- **输入**：Kafka Topic `traffic-raw` (来自 `pipeline-algo:9092`)
- **输出**：MyCat 8066 端口可查的 `traffic_data` 表（数据已自动分片）。

------



#### 📝 详细步骤指南

##### 第一步：部署双实例 MySQL (虚拟分片节点)



**背景**：为了演示“分布式存储”，我们需要两个数据库节点。由于只有一台服务器 `db-app`，我们需要在这台机器上同时运行**两个** MySQL 进程，分别监听 `3306` 和 `3307` 端口。

**⚠️ 关键警告 (参考朱少行报告 p.14)**：

> **严禁使用 MySQL 8.0**。Ubuntu 默认 apt 安装的是 8.0，它的大小写敏感配置和认证插件与 MyCat 1.6 兼容性极差。**必须手动安装 MySQL 5.7** (推荐使用 tar.gz 解压版或 docker)。

**操作指引**：

1. **环境清理**：如果 `db-app` 上已经有 MySQL 8.0，请先彻底卸载。

2. 多实例配置 (my.cnf)：

   你需要创建两个配置文件，例如 /etc/mysql/my3306.cnf 和 /etc/mysql/my3307.cnf。

   - **实例 1 (Shard 1)**:
     - Port: `3306`
     - Datadir: `/var/lib/mysql_3306`
     - Socket: `/tmp/mysql_3306.sock`
   - **实例 2 (Shard 2)**:
     - Port: `3307`
     - Datadir: `/var/lib/mysql_3307`
     - Socket: `/tmp/mysql_3307.sock`

3. 初始化与启动：

   分别初始化两个数据目录，并启动两个 mysqld 进程。

4. 建库建表：

   连接这两个实例（mysql -u root -P 3306 和 -P 3307），在两个实例中执行完全相同的 SQL：

   SQL

   ```
   CREATE DATABASE traffic_db;
   USE traffic_db;
   -- 建表结构需参考 API.md 和 字段说明
   CREATE TABLE traffic_data (
       id VARCHAR(50) PRIMARY KEY, -- GCXH
       kkmc VARCHAR(50),           -- 分片键
       hphm VARCHAR(20),
       gcsj DATETIME,              -- 关键：时间字段
       -- ... 其他字段 ...
       INDEX idx_gcsj (gcsj)       -- 必须加时间索引，否则查询慢
   );
   ```



##### 第二步：配置 MyCat 分片中间件



**背景**：MyCat 是数据库的“路由器”。它对外伪装成一个 MySQL，对内将数据切分到 3306 和 3307。

**参考资料**：`朱少行-后端.pdf` (p.6-8) - 他详细记录了 `schema.xml` 的配置。

**操作指引**：

1. **安装 JDK 1.8**：MyCat 运行依赖。

2. 配置 schema.xml (核心)：

   你需要修改 /usr/local/mycat/conf/schema.xml。

   - **定义数据节点 (dataNode)**：

     XML

     ```
     <dataNode name="dn1" dataHost="localhost1" database="traffic_db" />
     <dataNode name="dn2" dataHost="localhost1" database="traffic_db" />
     ```

   - 定义数据主机 (dataHost)：

     注意：这里通过 writeHost 的区分来连接不同的端口。

     XML

     ```
     <dataHost name="localhost1" ... >
         <writeHost host="hostM1" url="127.0.0.1:3306" user="root" password="...">
         </writeHost>
         </dataHost>
     ```

   - **定义逻辑表 (Table) 与分片规则**：

     XML

     ```
     <table name="traffic_data" dataNode="dn1,dn2" rule="sharding-by-kkmc" />
     ```

3. 配置 rule.xml (分片算法)：

   必须使用一致性哈希 (murmur)，这样数据分布最均匀。

   XML

   ```
   <tableRule name="sharding-by-kkmc">
       <rule>
           <columns>KKMC</columns>
           <algorithm>murmur</algorithm>
       </rule>
   </tableRule>
   ```

4. **启动与测试**：

   - 启动：`./mycat start`
   - 测试：用 Navicat 连接 `db-app:8066`。如果你往 `traffic_data` 插入一条数据，它应该只会出现在 3306 或 3307 中的一个，而不是两个都有。



##### 第三步：编写 Flink 入库程序 (Java)



**背景**：数据还在 Kafka 里，我们需要一个“搬运工”把它实时写入 MyCat。**注意：你的 Flink 代码将提交到 `pipeline-algo` 服务器运行，通过网络写入 `db-app`。**

**参考资料**：`杨研博-数据管道.pdf` (p.15-18) - 他的 `MysqlConnect` 类就是基于 Druid 连接池实现的，直接复用代码结构。

**代码逻辑 (Java)**：

1. **Source**: `KafkaSource` 连接 `localhost:9092` (因为 Flink 和 Kafka 同在 `pipeline-algo`)，消费 `traffic-raw`。
2. **Transform**: `Map` 或 `FlatMap`。
   - 将 `\t` 分隔的字符串切分。
   - **重要**：将 `GCSJ` (字符串) 解析为 Java `Timestamp` 对象，否则写入 MySQL `DATETIME` 字段会报错。
3. **Sink**: `RichSinkFunction` (JDBC Sink)。
   - **连接池**：使用 Alibaba Druid (`DruidDataSource`)。
   - **JDBC URL**: `jdbc:mysql://<db-app的内网IP>:8066/MYETCDB` (连接 MyCat，不是 MySQL)。
   - **SQL**: `INSERT INTO traffic_data (...) VALUES (...)`。
   - **Batch**: 建议设置 `batchSize` (如 50 条一刷)，提高吞吐量。

调试提示：

如果发现写入慢或报错，先检查 db-app 的防火墙是否开放了 8066 端口给 pipeline-algo。



##### 第四步：配置 MySQL 自动清理 (TTL)



**背景**：模拟数据是 7x24 小时生成的，如果不清理，你的磁盘几天就会爆满。

操作指引：

在 3306 和 3307 两个 MySQL 实例中，分别执行以下 SQL 开启定时任务：

SQL

```
-- 1. 开启事件调度器
SET GLOBAL event_scheduler = ON;

-- 2. 创建每日清理事件 (只保留最近3天)
CREATE EVENT auto_clean_traffic
ON SCHEDULE EVERY 1 DAY STARTS '2023-11-20 03:00:00'
DO
  DELETE FROM traffic_data WHERE gcsj < DATE_SUB(NOW(), INTERVAL 3 DAY);
```

------



#### 🗓️ 交付物清单 (Checklist)



1. [ ] **双端口 MySQL**: `netstat -ntlp | grep 3306` 和 `grep 3307` 均有进程。
2. [ ] **MyCat 服务**: 端口 `8066` 可连接，且 `SELECT * FROM traffic_data` 不报错。
3. [ ] **Flink 任务**: 提交到集群，状态为 `RUNNING`，且 MyCat 中数据量在持续增加。
4. [ ] **全局表数据**: 将同学 A 提供的 `kkmc_dimension.csv` 导入到了 MyCat 的 `kkmc_info` 表中 (用于后续关联查询)。

#### 参考资料

- **`朱少行-后端.pdf` (p. 6-8) 是你的“圣经”**。老师的要求**完美复刻**了这份报告中的“解决方案-2”（p. 8）。你必须100%实现 `sharding-by-murmur`。
- `朱少行-后端.pdf` (p. 14) 警告的 `MySQL 8.0` 坑，我们必须避免。
- `杨研博-数据管道.pdf` (p. 15-18) 依然是你 Flink Sink to MyCat 的代码参考。



### 阶段 3：算法与预警 (第2-7天，同学C) - (更新)

这是**阶段 3：算法实现与缓存部署**的详细执行指南，专为 **同学 C (算法工程师)** 准备。

根据 v2.0 架构，你的工作重心是“计算”和“模型”。你将在 `pipeline-algo` 服务器上运行计算任务，但将结果写入 `db-app` 服务器上的 Redis。

- **目标**：实现“套牌车实时报警”链路，并训练“流量预测”模型。
- **工期**：第 2 - 5 天
- **输入**：Kafka Topic `traffic-raw` (流数据) + `master_data_expanded.csv` (历史数据)。
- **输出**：
  1. Redis 中的 `traffic:warnings` 告警队列。
  2. 可供 Flask 加载的 Spark ML 模型文件。

------



#### 📝 详细步骤指南

##### 第一步：部署 Redis (在 db-app 服务器)

**背景**：为了让数据持久化与计算解耦，我们将 Redis 部署在存储节点 `db-app` 上。你的 Flink 任务将远程写入它。

**操作指引 (在 `db-app` 服务器执行)**：

1. **安装 Redis**：

   Bash

   ```
   sudo apt update
   sudo apt install redis-server
   ```

2. 配置远程访问：

   默认 Redis 只允许本地连接。你需要修改配置文件允许 pipeline-algo 连接。

   - 编辑配置：`sudo nano /etc/redis/redis.conf`
   - 修改绑定地址：找到 `bind 127.0.0.1 ::1`，改为 `bind 0.0.0.0` (允许所有IP连接，课设环境方便调试)。
   - (可选) 设置密码：找到 `requirepass`，取消注释并设一个简单密码（如 `123456`），或者保持无密码。

3. **重启并验证**：

   - 重启：`sudo systemctl restart redis-server`
   - 测试：在 `db-app` 上输入 `redis-cli ping`，应返回 `PONG`。



##### 第二步：编写 Flink 实时预警程序 (Java)



**背景**：这是本项目的**最大亮点**。你需要用 Flink 的**状态编程 (Stateful Processing)** 来检测异常。

逻辑核心：

“如果同一车牌 (keyBy HPHM)，在 5分钟内 出现在了 两个不同的卡口，且距离不可能在这么短时间内到达（简化逻辑：只要卡口不同就视为嫌疑，或者你可以维护一个简单的距离表），则判定为套牌车。”

**参考资料**：`杨再润-组长-预测与报警(LSTM-Flink).pdf.pdf` (p.3-4) - 重点参考他的 Flink 逻辑。

**代码逻辑 (Java)**：

1. **Source**: 连接 Kafka `localhost:9092`，消费 `traffic-raw`。
2. **Transform**: 解析字符串，提取 `HPHM` (车牌), `KKMC` (卡口), `GCSJ` (时间)。
3. **KeyBy**: 按车牌号分组 -> `.keyBy(data -> data.hphm)`。
4. **Process (核心)**: 使用 `KeyedProcessFunction`。
   - **定义状态**: `ValueState<TrafficRecord> lastAppearanceState;` (存储上一次该车牌出现的记录)。
   - **处理逻辑 (`processElement`)**:
     1. 读取 `lastAppearanceState`。
     2. 如果状态为空 (第一次出现)，更新状态为当前记录。
     3. 如果状态不为空：
        - 计算当前时间与 `lastAppearanceState` 时间的差值。
        - **判定规则**：如果 `时间差 < 5分钟` **且** `当前卡口 != 上次卡口`：
          - **触发告警**：生成一条告警字符串 (JSON格式)。
          - `out.collect(alarmJson);`
        - 更新状态为当前记录。
5. **Sink**: `RedisSink` (使用 `flink-connector-redis`)。
   - **Host**: `<db-app的内网IP>` (注意不是 localhost)。
   - **Port**: `6379`。
   - **Command**: `LPUSH` (将告警推入 List 头部)。
   - **Key**: `traffic:warnings` (严格遵守 API 文档定义的 Key)。

交付物验证：

在 pipeline-algo 运行 Flink 任务，然后在 db-app 上运行 redis-cli lrange traffic:warnings 0 -1，看是否有数据进来。



##### 第三步：Spark ML 离线预测 (PySpark)



**背景**：利用历史数据训练一个简单的线性回归模型，预测未来的流量。这个模型不需要太复杂，关键是**流程跑通**。

**操作指引 (在 `pipeline-algo` 服务器执行)**：

1. **准备环境**：确保已安装 Spark 和 PySpark (`pip install pyspark`)。

2. **编写脚本 `train_model.py`**：

   Python

   ```
   from pyspark.sql import SparkSession
   from pyspark.ml.feature import VectorAssembler, StringIndexer
   from pyspark.ml.regression import LinearRegression
   from pyspark.ml import Pipeline
   
   # 1. 初始化
   spark = SparkSession.builder.appName("TrafficPrediction").getOrCreate()
   
   # 2. 读取数据 (同学A提供的清洗后数据)
   df = spark.read.csv("master_data_expanded.csv", header=True, inferSchema=True)
   
   # 3. 特征工程 (简化版)
   # 假设我们要用 'hour' (小时) 和 'kkmc' (卡口) 来预测 'count' (流量)
   # 你需要先对数据进行预处理，聚合成 [kkmc, hour, count] 的格式
   
   # 将字符串类型的卡口名转换为数值索引
   indexer = StringIndexer(inputCol="KKMC", outputCol="kkmc_index")
   
   # 组装特征向量
   assembler = VectorAssembler(
       inputCols=["kkmc_index", "hour", "is_weekend"], 
       outputCol="features"
   )
   
   # 4. 定义模型
   lr = LinearRegression(featuresCol="features", labelCol="traffic_count")
   
   # 5. 构建 Pipeline 并训练
   pipeline = Pipeline(stages=[indexer, assembler, lr])
   model = pipeline.fit(training_data)
   
   # 6. 保存模型 (关键交付物)
   model.save("traffic_flow_model") 
   ```

3. 执行训练：

   运行脚本，生成 traffic_flow_model 文件夹。



##### 第四步：移交交付物给同学 D



**这一步至关重要**，因为现在 API 由 D 负责开发，他需要你的成果才能写代码。

1. **Redis 信息**：

   - 告诉 D：Redis 地址是 `localhost:6379` (对 D 来说是本机)，Key 是 `traffic:warnings`，数据结构是 List。

2. **模型文件**：

   - 将训练好的 `traffic_flow_model` 文件夹打包。

   - 发送给 D (或者拷贝到 `db-app` 服务器的指定目录)，并告诉 D 如何用 PySpark 加载这个模型：

     Python

     ```
     # D 的代码示例
     from pyspark.ml import PipelineModel
     model = PipelineModel.load("./traffic_flow_model")
     result = model.transform(input_data)
     ```

------



#### 🗓️ 交付物清单 (Checklist)



1. [ ] **Redis 服务**: 在 `db-app` 上运行，且 `pipeline-algo` 可以远程连接。
2. [ ] **Flink 告警任务**: 代码中实现了“同车不同卡口”检测逻辑，状态为 `RUNNING`。
3. [ ] **告警数据落地**: Redis 中 `traffic:warnings` 列表里有实时的 JSON 数据。
4. [ ] **预测模型**: `traffic_flow_model` 文件夹已生成，并验证可以被加载。

#### **参考资料**：

- **`杨再润-组长.pdf` (p. 3-4, 7) 是你的“圣经”**。他的 `Flink 流量监测 -> Redis -> Flask 预警` 架构 (`p. 3, 7`)，就是你要实现的核心链路。
- **注意**：`杨再润-组长.pdf` (p. 5, 8-9) 中所有关于 `HBase` 的内容，我们**正式忽略**，这对你是个好消息。

### 阶段4,：后端api开发

这是**阶段 4：后端 API 服务开发**的详细执行指南，专为 **同学 D (后端开发工程师)** 准备。

在 v2.0 架构中，你是连接底层数据（B和C的成果）与前端展示（A的成果）的关键枢纽。你的工作是屏蔽底层的复杂性（分片数据库、缓存队列、机器学习模型），对外暴露标准的 HTTP JSON 接口。

- **目标**：搭建 Flask 服务，实现 `API.md` 中定义的 **8个核心接口**。
- **工期**：第 4 - 8 天
- **工作区**：`db-app` 服务器 (端口 `5000`)
- **输入**：
  1. MyCat 数据 (`traffic_data` 表, 端口 8066)
  2. Redis 数据 (`traffic:warnings` 队列, 端口 6379)
  3. Spark ML 模型文件 (`traffic_flow_model` 文件夹)
- **输出**：一个支持跨域访问 (CORS) 的 RESTful API 服务。

------



#### 📝 详细步骤指南

##### 第一步：搭建 Flask 环境与基础架构



**背景**：你需要一个轻量级的 Web 框架来托管 API。

**操作指引 (在 `db-app` 服务器执行)**：

1. 安装依赖：

   你需要安装连接 MySQL、Redis 和运行 Spark 模型所需的库。

   Bash

   ```
   pip install flask flask-cors pymysql redis pyspark numpy pandas
   ```

   *注意：运行 PySpark 需要服务器上配置好 Java 环境（B同学搭建 MyCat 时应该已经装了 JDK 1.8，确认一下 `java -version`）。*

2. 项目结构初始化：

   建议创建如下目录结构，保持代码整洁：

   Plaintext

   ```
   /home/student_d/backend/
   ├── app.py              # 入口文件
   ├── config.py           # 数据库配置
   ├── db_utils.py         # MyCat 连接与 SQL 拼接工具 (参考朱少行报告)
   ├── model_loader.py     # Spark 模型加载器
   └── traffic_flow_model/ # C同学发给你的模型文件夹
   ```

3. 配置 app.py (核心)：

   初始化 Flask 并配置跨域，这是前端能访问你的关键。

   Python

   ```
   from flask import Flask
   from flask_cors import CORS
   
   app = Flask(__name__)
   # 允许所有来源跨域，生产环境不推荐，但课设环境必须这样配，否则前端A会报错
   CORS(app, resources={r"/api/*": {"origins": "*"}})
   
   if __name__ == '__main__':
       # 监听 0.0.0.0 允许外部访问
       app.run(host='0.0.0.0', port=5000, debug=True)
   ```



##### 第二步：实现业务查询接口 (对接 MyCat)



**背景**：前端的大屏图表和查询页面需要从数据库读数据。你需要连接 B 同学搭建的 MyCat（端口 8066），而不是直接连 MySQL。

**参考资料**：`朱少行-后端(Flask)与数据库(MyCat).pdf.pdf` (p. 9-13) - 重点参考他的 `utils` 类（动态 SQL 拼接）和 `MyDb` 类。

**代码逻辑 (Python)**：

1. **数据库连接 (`config.py`)**：

   Python

   ```
   DB_CONFIG = {
       'host': '127.0.0.1', # MyCat 在本机
       'port': 8066,        # 必须是 8066，不能是 3306
       'user': 'root',
       'password': '123456', # MyCat server.xml 中配置的密码
       'db': 'MYETCDB',      # MyCat 逻辑库名
       'charset': 'utf8mb4'
   }
   ```

2. **实现 `[接口 1] 交互式查询` (`POST /api/query`)**：

   - **难点**：前端传来的条件是动态的（可能有车牌，可能没有；可能有时间范围，可能没有）。
   - **逻辑**：
     1. 接收 JSON 参数。
     2. 构建 SQL `WHERE` 子句。例如：如果 `hphm` 不为空，则 `sql += " AND hphm LIKE '%{}%'".format(hphm)`。
     3. **分页**：必须加上 `LIMIT {offset}, {size}`。
     4. 执行 SQL，返回结果。

3. **实现 `[接口 2-6] 大屏统计接口`**：

   - 这些都是聚合查询 (`GROUP BY`)。

   - **示例 (卡口流量 Top10)**：

     SQL

     ```
     SELECT kkmc as name, COUNT(*) as value 
     FROM traffic_data 
     GROUP BY kkmc 
     ORDER BY value DESC 
     LIMIT 10;
     ```

   - **注意**：`map_data` (地图) 和 `vehicle_type` (车型) 也是类似的 `GROUP BY` 语句。



##### 第三步：实现实时告警接口 (对接 Redis)



**背景**：C 同学的 Flink 任务已经把告警推到了 Redis 的 List 中，你需要取出来给前端展示。

**操作指引**：

1. **连接 Redis**：

   Python

   ```
   import redis
   r = redis.Redis(host='127.0.0.1', port=6379, decode_responses=True)
   ```

2. **实现 `[接口 7] 实时套牌车告警` (`GET /api/warnings/realtime`)**：

   - **逻辑**：前端大屏通常需要显示“最新”的几条告警。

   - **操作**：使用 `LRANGE` 命令读取 List 的前 10 条。

     Python

     ```
     # 获取最新的 10 条告警
     warnings = r.lrange("traffic:warnings", 0, 9)
     # warnings 是一个 JSON 字符串列表，直接返回给前端即可，或者解析后重新封装
     ```



##### 第四步：实现流量预测接口 (对接 Spark 模型)



**背景**：C 同学训练好了模型，你需要加载它来预测未来流量。

**⚠️ 性能关键**：SparkSession 的启动非常慢（可能要几秒到十几秒）。**严禁**在每次 API 请求时才创建 SparkSession。**必须**在 Flask 启动时（全局）初始化 Spark 和加载模型。

**代码逻辑**：

1. **全局加载模型 (`app.py` 顶部)**：

   Python

   ```
   from pyspark.sql import SparkSession
   from pyspark.ml import PipelineModel
   
   # 1. 初始化 Spark (只做一次)
   spark = SparkSession.builder \
       .appName("FlaskPredictionService") \
       .master("local[*]") \
       .config("spark.driver.memory", "1g") \
       .getOrCreate()
   
   # 2. 加载模型 (C同学提供的文件夹路径)
   try:
       traffic_model = PipelineModel.load("./traffic_flow_model")
       print(">>> 模型加载成功！")
   except Exception as e:
       print(f">>> 模型加载失败: {e}")
   ```

2. **实现 `[接口 8] 未来流量预测` (`GET /api/predict/flow`)**：

   - **接收参数**：`kkmc` (卡口), `hour` (小时)。

   - **构造输入**：创建一个包含特征的 DataFrame（需与 C 同学训练时的输入格式一致，例如 `kkmc_index`, `hour`, `is_weekend`）。

   - **预测**：

     Python

     ```
     # 构造单行数据
     input_df = spark.createDataFrame([(kkmc_val, int(hour_val), ...)], schema=...)
     # 预测
     prediction = traffic_model.transform(input_df)
     result = prediction.select("prediction").first()[0]
     ```

   - **返回**：`{"predicted_flow": int(result)}`。



#### 🗓️ 交付物清单 (Checklist)

1. [ ] **Flask 服务运行中**: `netstat -ntlp | grep 5000` 有进程。
2. [ ] **MyCat 连通性**: 调用 `/api/stats/kkmc_count` 能返回 JSON 数据。
3. [ ] **Redis 连通性**: 调用 `/api/warnings/realtime` 能返回 C 同学生成的告警数据。
4. [ ] **Spark 模型加载**: Flask 启动日志显示“模型加载成功”，且预测接口能在 1秒内返回结果（第一次可能稍慢，后续应很快）。
5. [ ] **API 文档对齐**: 你的接口路径、参数和返回格式与 A 同学写的 `API.md` **完全一致**。



#### 📚 参考资料

- **`项目技术接口规范与交付物定义.md` (即 API.md)**：这是你的开发圣经，字段名一个字母都不能错，否则前端 A 会找你麻烦。
- **`朱少行-后端(Flask)与数据库(MyCat).pdf.pdf`**：
  - **p. 9-13**: 参考他的 Flask 代码结构，特别是如何处理 MyCat 的查询。
  - **p. 14**: 如果遇到查询不到刚插入的数据，参考他提到的“延时查询”思路（虽然在演示场景下，数据一直在流转，通常不需要特意延时）。
- **`杨再润-组长.pdf` (p. 7)**：参考他设计的后端 API 接口风格。

### 阶段 5：前端集成与联调 (第3-10天，A)

- **行动 (我)**：(不变) 我们的 API 规范没有变，B 和 C 的 API Host 地址也没有变。
  1. Mock 开发（`刘冰峰-前端.pdf` 和 `高成-前端.pdf` 是你的 UI 和交互参考）。
  2. 联调 `http://db-app:5000` (B 的 API) 和 `http://pipeline-algo:5001` (C 的 API)。
- **参考资料**：
  - **`react-big-data` (代码)**：这是“黄金宝库”。不要学它的 API 设计，但要**100%“扒”它**的 Echarts 配置（`src/components/*/charts/options.js`）和 UI 布局（`src/routes/IndexPage.jsx`）。
  - `刘冰峰-前端.pdf` (p. 16)：注意他提到的“卡壳”和“并发性问题”。如果我们的 API 慢，要及时和 B 沟通（他可能需要实现 `朱少行-后端.pdf` p. 14 的“延时查询”）。
  - `高成-前端.pdf` (p. 4-6)：这是你“交互式查询”页面的唯一参考，学习他如何实现“后端查询”和“分页”。