好的，我们已经明确了老师的所有要求，特别是“HBase或MyCat二选一”、“虚拟节点=数据分片”和“AI扩充数据”这几点。

这太好了！我们的技术路线现在100%清晰了。**我们正式决定：放弃HBase，集中精力主攻 MyCat 数据分片，并升级数据模拟方案。**

这是我们最新的、也是最终的**《课设项目策划书 v1.1 (最终版)》**。它已经集成了我们所有的讨论和老师的最新反馈，是我们4个人未来几周的“唯一行动指南”。

------



### 课设项目策划书：《高速公路实时车流监控与套牌车预警平台》v1.1



#### 1. 🎯 项目概述

本项目旨在利用你提供的 `交通流量数据字段说明.png` 所示的高速公路卡口过车数据，构建一个集**数据存储、实时监控、交互查询、实时预警**于一体的大数据管理平台。

我们将复用往届学长的经典技术架构（Flume+Kafka+Flink，参考 `image_abe064.png`），但业务逻辑将**完全围绕我们的“卡口数据”和“套牌车检测”扩展要求展开**。

**核心功能 (已确认):**

1. **数据大屏**：动态展示全市车流、卡口流量排名、24小时流量曲线、车辆来源地图炮。（参考 `刘冰峰-前端.pdf`）
2. **交互查询**：按卡口、车牌、时间等多维度组合查询过车记录。（参考 `高成-前端.pdf`）
3. **实时预警 (项目亮点)**：实时检测“同一车牌在短时间内出现在不同卡口”的套牌车嫌疑行为。（参考 `大数据存储案例设计要求.doc`）



#### 2. 💻 核心技术栈 (最终版)



- **数据采集**：Python (模拟数据流 + **AI数据修复与扩充**)
- **数据缓存**：Kafka
- **数据存储**：MySQL 5.7 (**单机双节点/数据分片**), **MyCat** (核心), Redis
- **实时计算**：Flink
- **离线计算**：Spark ML
- **后端服务**：Python Flask
- **前端展示**：Vue / React, Echarts
- **(已废弃)**：HBase



#### 3. 👥 4人团队分工 (RACI) - (最终版)



| **角色**            | **负责人**  | **核心职责 (Responsible)**                                   | **关键交付物**                                               |
| ------------------- | ----------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **前端总负责**      | **我 (你)** | 1. 制定 API 规范 (`API.md`) 2. 开发数据大屏 3. 开发交互式查询页面 | 1. `API.md` 接口文档 2. Vue/React 前端项目                   |
| **数据管道工**      | **同学A**   | 1. **AI数据预处理（修复与扩充）** 2. 模拟实时数据流 3. 搭建 Flume -> Kafka 管道 | 1. `preprocess.py` 脚本 2. `simulate.py` 脚本 3. 稳定的 `traffic-raw` Topic |
| **后端与存储 (主)** | **同学B**   | 1. 搭建 MyCat+MySQL **(数据分片)** 2. Flink 消费数据入库 MyCat 3. 开发“业务类” Flask API | 1. `schema.xml` (MyCat配置) 2. Flink to MyCat (Java) 3. Flask API (业务) |
| **算法与预警 (辅)** | **同学C**   | 1. Flink 实时检测套牌车 (to Redis) 2. 训练 Spark ML 预测模型 3. 开发“算法类” Flask API **(已移除：HBase Sink)** | 1. Flink to Redis (Java) 2. Spark ML 模型文件 3. Flask API (算法) |



#### 4. 🛰️ 基础设施部署方案 (2台服务器 - 虚拟节点版) - (已确认)



根据老师的确认，我们采用**2台服务器**方案，在 `db-app` 服务器上使用“虚拟节点”实现数据分片。

| **服务器**   | **命名**        | **运行的服务 (负责人)**                                      |
| ------------ | --------------- | ------------------------------------------------------------ |
| **服务器 1** | `pipeline-algo` | **Zookeeper** (A) **Kafka** (A) **Flume** (A) **Flink** (B, C) **Redis** (C) **Spark** (C) **Flask (算法API) @ 5001** (C) |
| **服务器 2** | `db-app`        | **MySQL (分片1) @ 3306** (B) **MySQL (分片2) @ 3307** (B) **MyCat @ 8066** (B) **Flask (业务API) @ 5000** (B) |

------



#### 5. 📖 详细执行计划 (Step-by-Step) v1.1





##### 阶段 0：项目“握手协议” (第1天，我)



- **目标**：定义前后端“契约”，实现解耦。
- **行动 (我)**：我（前端）**必须立刻**编写一份 `API.md` 接口文档，发给同学 B 和 C。这份文档就是我们之前讨论的**《项目技术接口规范与交付物定义》**。
- **参考资料**：
  - `刘冰峰-前端.pdf` (第 9-12 页)：参考他为大屏图表定义了哪些数据格式。
  - `朱少行-后端.pdf` (第 9-13 页)：参考他的 API 设计（`solution` 类）。



##### 阶段 1：数据管道打通 (第1-3天，同学A) - (重大更新)



- **目标**：提供一个稳定的、**经过AI修复和属性扩充的** Kafka Topic (`traffic-raw`)。

- **行动 (同学A)**：

  1. **数据准备**：将 `202312` 和 `202401` 文件夹下的所有 CSV/XLSX 文件合并。

  2. **(新增) AI数据修复 (`preprocess.py`)**：

     - **目标**：解决12月数据脏（如`KKMC`为空）导致清洗后数据不够用的问题。

     - **行动**：编写一个一次性运行的 Python 脚本 (`preprocess.py`)。

     - **逻辑**：

       ```Python
       import pandas as pd
       import random
       
       df = pd.read_csv("merged_data.csv")
       
       # 1. AI 补充（修复空值）
       # 获取所有非空的卡口名称列表
       valid_kkmc = df[df['KKMC'].notnull()]['KKMC'].unique() 
       # 对 KKMC 列中的空值 (NaN)，从 valid_kkmc 列表中随机选一个填充
       df['KKMC'] = df['KKMC'].apply(lambda x: random.choice(valid_kkmc) if pd.isnull(x) else x)
       
       # (对 HPHM, CLPPXH 等字段也执行类似的“修复”逻辑)
       # ...
       df.to_csv("master_data_cleaned.csv", index=False)
       ```

  3. **(更新) 模拟脚本 (`simulate.py`)**：

     - **目标**：读取**修复后**的数据，并**扩充属性**（如天气），模拟“每秒50条”的实时流。

     - **行动**：`pandas` 读取 `master_data_cleaned.csv`。

     - **逻辑**：

       Python

       ```
       import pandas as pd
       import time
       import datetime
       
       df = pd.read_csv("master_data_cleaned.csv")
       while True:
           for index, row in df.iterrows():
               # 1. 替换时间戳
               row['GCSJ'] = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
       
               # 2. AI 扩充（增加新属性 'WEATHER'）
               current_month = datetime.datetime.now().month
               weather = "Sunny" # 默认
               if current_month == 12 or current_month == 1:
                   weather = "Snowy"
               elif current_month in [6, 7, 8]:
                   weather = "Rainy"
       
               # 3. 构造新的日志行（9个字段），用 Tab 分隔
               log_line = f"{row[0]}\t{row[1]}\t...\t{row[7]}\t{weather}\n"
       
               # 4. 写入日志
               with open("/var/log/traffic.log", "a") as f:
                   f.write(log_line)
               time.sleep(1.0 / 50.0) # 控制速率
       ```

  4. **Flume 配置**：(不变) `TAILDIR` 监控 `/var/log/traffic.log`，Sink 到 `traffic-raw`。

- **参考资料**：

  - `杨研博-数据管道.pdf` (第 4-7, 13 页) 依然是 ZK, Kafka, Flume 的环境配置圣经。
  - `杨研博-数据管道.pdf` (第 9-11 页) 的 Python 脚本**只能参考思路**，我们的 `preprocess.py` 和 `simulate.py` 逻辑比他的更复杂（多了AI修复和扩充）。



##### 阶段 2：后端存储与业务 API (第2-7天，同学B) - (重大更新)



- **目标**：搭建**数据分片**集群，并提供 API。

- **行动 (同学B)**：

  1. **(更新) 搭建数据库 (数据分片)**：

     - **目标**：在 `db-app` 服务器上，安装**两个**独立的 MySQL 5.7 实例（**非主从**），即“虚拟节点”。
     - **行动**：搜索“Linux 单机 MySQL 多实例部署”。
     - 实例1：配置 `my-master.cnf`，使用 `port=3306`, `datadir=/var/lib/mysql_3306`。
     - 实例2：配置 `my-slave.cnf`，使用 `port=3307`, `datadir=/var/lib/mysql_3307`。
     - 在**两个**实例上都执行 `CREATE DATABASE traffic_db;`

  2. **(更新) 配置 MyCat (`schema.xml`)**：

     - **目标**：配置 MyCat，让它把数据**切分**到 `3306` 和 `3307` 两个节点。

     - **`dataNode` 定义**：

       XML

       ```
       <dataNode name="dn1" dataHost="127.0.0.1" database="traffic_db" port="3306" ... />
       <dataNode name="dn2" dataHost="127.0.0.1" database="traffic_db" port="3307" ... />
       ```

     - **`tableRule` (核心)**：

       XML

       ```
       <tableRule name="sharding-by-kkmc">
         <rule>
           <columns>KKMC</columns> <algorithm>murmur</algorithm>
         </rule>
       </tableRule>
       ```

     - **`table` 定义**：

       XML

       ```
       <table name="traffic_data" dataNode="dn1,dn2" rule="sharding-by-kkmc" />
       ```

  3. **Flink 消费 (Java/Scala)**：(不变) `Source` 从 `traffic-raw` 读数据 (9个字段)，`Sink` 到 MyCat 的 `8066` 端口。

  4. **Flask API**：(不变) `app_B.py` 连接 MyCat `8066` 端口。

- **参考资料**：

  - **`朱少行-后端.pdf` (p. 6-8) 是你的“圣经”**。老师的要求**完美复刻**了这份报告中的“解决方案-2”（p. 8）。你必须100%实现 `sharding-by-murmur`。
  - `朱少行-后端.pdf` (p. 14) 警告的 `MySQL 8.0` 坑，我们必须避免。
  - `杨研博-数据管道.pdf` (p. 15-18) 依然是你 Flink Sink to MyCat 的代码参考。



##### 阶段 3：算法与预警 (第2-7天，同学C) - (更新)



- **目标**：(HBase 任务移除) 100% 聚焦 Flink 实时告警和 Spark ML 预测。
- **行动 (同学C)**：
  1. **Flink 实时预警 (Java/Scala)**：(不变)
     - `Source` 从 `traffic-raw` 读数据。
     - `keyBy(HPHM).process(TaopaiDetector)` (状态编程)。
     - `Sink` 到 `Redis` (Key: `traffic:warnings`, Type: `List`)。
  2. **Spark ML 离线预测**：(不变)
     - 读取同学A产出的 `master_data_cleaned.csv`。
     - 训练 `LinearRegression` 模型并保存 `traffic_model`。
  3. **Flask API**：(不变)
     - `app_C.py` 运行在 `pipeline-algo:5001`。
     - `/api/warnings/realtime` (从 Redis 读)。
     - `/api/predict/flow` (加载 Spark 模型)。
- **参考资料**：
  - **`杨再润-组长.pdf` (p. 3-4, 7) 是你的“圣经”**。他的 `Flink 流量监测 -> Redis -> Flask 预警` 架构 (`p. 3, 7`)，就是你要实现的核心链路。
  - **注意**：`杨再润-组长.pdf` (p. 5, 8-9) 中所有关于 `HBase` 的内容，我们**正式忽略**，这对你是个好消息。



##### 阶段 4：前端集成与联调 (第3-10天，我)



- **行动 (我)**：(不变) 我们的 API 规范没有变，B 和 C 的 API Host 地址也没有变。
  1. Mock 开发（`刘冰峰-前端.pdf` 和 `高成-前端.pdf` 是你的 UI 和交互参考）。
  2. 联调 `http://db-app:5000` (B 的 API) 和 `http://pipeline-algo:5001` (C 的 API)。
- **参考资料**：
  - **`react-big-data` (代码)**：这是“黄金宝库”。不要学它的 API 设计，但要**100%“扒”它**的 Echarts 配置（`src/components/*/charts/options.js`）和 UI 布局（`src/routes/IndexPage.jsx`）。
  - `刘冰峰-前端.pdf` (p. 16)：注意他提到的“卡壳”和“并发性问题”。如果我们的 API 慢，要及时和 B 沟通（他可能需要实现 `朱少行-后端.pdf` p. 14 的“延时查询”）。
  - `高成-前端.pdf` (p. 4-6)：这是你“交互式查询”页面的唯一参考，学习他如何实现“后端查询”和“分页”。